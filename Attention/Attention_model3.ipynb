{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../data/Rail_data.csv')\n",
    "df = pd.read_csv('C:/Users/AML2/Desktop/TIL/data/Rail_data.csv')\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_col = ['air_temp','TSI','azimuth','altitude','solar_rad','High_solar_rad', 'casi', 'humidity', 'rain', 'wind_speed','wind_direction','rail_direction']\n",
    "# df[scaled_col]= scaler.fit_transform(df[scaled_col])\n",
    "\n",
    "# scaler1 = MinMaxScaler()\n",
    "# df['rail_temp'] = scaler1.fit_transform(df['rail_temp'].values.reshape(-1,1))\n",
    "\n",
    "X = df.iloc[:,:12].values\n",
    "y = df.iloc[:,12].values\n",
    "\n",
    "def sequence_data(X,y, sequence_size):\n",
    "    x_seq = []\n",
    "    y_seq = []\n",
    "    for idx in range(len(X) - sequence_size): #len(X)가 7000이고 seq_size가 5라면?\n",
    "        x_seq.append(X[idx:idx + sequence_size]) #1에서 5일차의 값을 가지고 6일차를 예측하기\n",
    "        y_seq.append(y[idx:idx + sequence_size])\n",
    "        \n",
    "    return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "split = 48052\n",
    "sequence_length = 5\n",
    "X_seq, y_seq = sequence_data(X, y, sequence_length)\n",
    "\n",
    "X_train_seq = X_seq[:split]\n",
    "y_train_seq = y_seq[:split]\n",
    "X_test_seq = X_seq[split:]\n",
    "y_test_seq = y_seq[split:]\n",
    "\n",
    "\n",
    "train_DS = TensorDataset(X_train_seq, y_train_seq)\n",
    "test_DS = TensorDataset(X_test_seq, y_test_seq)\n",
    "\n",
    "train_DL = DataLoader(train_DS, batch_size = 16)\n",
    "test_DL = DataLoader(test_DS, batch_size = 16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48052, 5, 12])\n",
      "torch.Size([48052, 5])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq.shape)\n",
    "print(y_train_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # input_dim >> emb_dim으로 embedding\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        # emb_dim >> enc_hid_him(hidden state) 뽑아내기\n",
    "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        # enc_hid_dim >> dec_hid_dim으로 선형변화\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 일정 부분의 연결을 끊어서 성능을 올림\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x)) # x를 변수들의 dataset으로 보기?\n",
    "        # embedded: x len, batch size, emb dim >> x len = sequence_length / batch_size는 임의로 설정 maybe 128?\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]), dim=1))) # hidden에서의 마지막 step 가져오기\n",
    "        # outputs: x- len, batch size, enc_hid_dim \n",
    "        # hidden: batch size, dec_hid_dim\n",
    "        return outputs, hidden\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim*2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, bias=False)\n",
    "        # dot-product를 사용하지 않고 attention score 계산?\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # encoder_outputs: x len, batch size, enc_hid_dim\n",
    "        # hidden: batch size, dec_hid_dim\n",
    "        batch_size = encoder_outputs.shape[1] # batch size\n",
    "        x_len = encoder_outputs.shape[0] # x len\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, x_len, 1) \n",
    "        # hidden: batch size, x len, dec_hid_dim\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        # encoder_outputs: batch size, x len, enc_hid_dim \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "        # energy: batch size, x len, dec_hid_dim\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention: batch size, x len\n",
    "        return F.softmax(attention, dim=1)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim*2) + emb_dim, output_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim*2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        input = input.unsqueeze(0) # y가 될 것으로 보임?\n",
    "        # input: batch size\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded: 1, batch size, emb_dim\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a: batch size, x len\n",
    "        a = a.unsqueeze(1)\n",
    "        # a: batch size, 1, x len\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        # encoder_outputs: batch size, x len, enc_hid_dim\n",
    "        weighted = torch.bmm(a, encoder_outputs) # 행렬곱 >> (1*x len) X (x len * enc_hid_dim)\n",
    "        # weighted: batch size, 1, enc_hid_dim*2\n",
    "        weighted = weighted.permute(1,0,2)\n",
    "        # weighted: 1, batch size, enc_hid_dim*2\n",
    "        rnn_input = torch.cat((embedded, weighted), dim =2)\n",
    "        # rnn_input: 1, batch size, emb_dim+(enc_hid_dim*2)\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0)) \n",
    "        # output: 1, batch size, dec_hid_dim\n",
    "        # hidden: 1, batch size, dec_hid_dim\n",
    "        assert (output == hidden).all() # 괄호 안 진위여부 확인\n",
    "        \n",
    "        embedded = embedded.squeeze(0) # embedded: batch size, emb_dim\n",
    "        output = output.squeeze(0)     # output: batch size, dec_hid_dim\n",
    "        weighted = weighted.squeeze(0) # weighted: batch size, enc_hid_dim\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # dim=1에서의 size는 다른데 torch.cat 가능?\n",
    "        # prediction: batch size, output dim\n",
    "        return prediction, hidden.squeeze(0)\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = x.shape[1]\n",
    "        trg_len = y.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # batch size?\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(x)\n",
    "                \n",
    "        \n",
    "        input = y[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            \n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = y[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = \n",
    "output_dim = \n",
    "enc_emb_dim = 256\n",
    "dec_emb_dim = 256\n",
    "enc_hid_dim = 512\n",
    "dec_hid_dim = 512\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "attn = Attention(enc_hid_dim, dec_hid_dim)\n",
    "enc = Encoder(input_dim, enc_emb_dim, enc_hid_dim, dec_hid_dim, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, enc_hid_dim, dec_hid_dim, dec_dropout, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     for name, param in m.named_parameters():\n",
    "#         if 'weight' in name:\n",
    "#             nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "#         else:\n",
    "#             nn.init.constant_(param.data, 0)\n",
    "            \n",
    "# model.apply(init_weights)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #gpu 활성화 확인\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "loss_graph = []\n",
    "n = len(train_DL)\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0\n",
    "    for data in train_DL:\n",
    "        seq, target = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        out = model(seq)\n",
    "        loss = criterion(out, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    loss_graph.append(running_loss/n)\n",
    "    if epoch % 20 == 0:\n",
    "        print(\"[epoch: %d] loss : %.4f\" %(epoch,running_loss/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatdata = torch.utils.data.ConcatDataset([train_DS, test_DS])\n",
    "data_loader = DataLoader(dataset=concatdata, batch_size= 16)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = []\n",
    "    for data in data_loader:\n",
    "        seq, target = data[0].to(device), data[1].to(device)\n",
    "        out = model(seq)\n",
    "        pred += out.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
