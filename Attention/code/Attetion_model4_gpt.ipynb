{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10 # 원하는 sequence_length 설정\n",
    "batch_size = 36 # 원하는 batch_size 설정 >> 6시간 기준\n",
    "hidden_size = 16 # 원하는 hidden_size 설정\n",
    "input_size = 11 # 사용하는 feature의 수\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../data/Rail_data.csv')\n",
    "df = pd.read_csv('../../data/Rail_data_split.csv')\n",
    "df1 = pd.read_csv('../../data/Rail_data_back_split.csv')\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_col = ['air_temp','TSI','azimuth','altitude','solar_rad','High_solar_rad', 'casi', 'humidity', 'rain', 'wind_speed','wind_direction','rail_direction']\n",
    "# df[scaled_col]= scaler.fit_transform(df[scaled_col])\n",
    "\n",
    "# scaler1 = MinMaxScaler()\n",
    "# df['rail_temp'] = scaler1.fit_transform(df['rail_temp'].values.reshape(-1,1))\n",
    "\n",
    "df = df.astype({'solar_rad': 'float64'})\n",
    "df = df.astype({'High_solar_rad': 'float64'})\n",
    "df = df.astype({'casi': 'float64'})\n",
    "df = df.astype({'humidity': 'float64'})\n",
    "df = df.astype({'wind_speed': 'float64'})\n",
    "df = df.drop(['rail_direction'], axis=1)\n",
    "# int type >> float type\n",
    "\n",
    "df1 = df1.astype({'solar_rad': 'float64'})\n",
    "df1 = df1.astype({'High_solar_rad': 'float64'})\n",
    "df1 = df1.astype({'casi': 'float64'})\n",
    "df1 = df1.astype({'humidity': 'float64'})\n",
    "df1 = df1.astype({'wind_speed': 'float64'})\n",
    "df1 = df1.drop(['rail_direction'], axis=1)\n",
    "\n",
    "X = df.iloc[:,:11].values\n",
    "y = df.iloc[:,11].values\n",
    "\n",
    "X1 = df1.iloc[:,:11].values\n",
    "y1 = df1.iloc[:,11].values\n",
    "\n",
    "def sequence_data(X,y, sequence_size): # 원하는 sequence에 따라 데이터 분리\n",
    "    x_seq = []\n",
    "    y_seq = []\n",
    "    for idx in range(len(X) - sequence_size): #len(X)가 7000이고 seq_size가 5라면?\n",
    "        x_seq.append(X[idx:idx + sequence_size]) # sequence_lengh개씩 특성들을 모두 묶음 >> shape: 5, 11\n",
    "        y_seq.append(y[idx + sequence_size])     # x에 따른 온도들을 묶음 >> shape: 5, 1\n",
    "        \n",
    "    return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "X_seq, y_seq = sequence_data(X, y, sequence_length)\n",
    "\n",
    "\n",
    "X_seq, y_seq = sequence_data(X, y, sequence_length) # 원하는 sequence_length에 따라 데이터 묶기\n",
    "X1_seq, y1_seq = sequence_data(X1, y1, sequence_length)\n",
    "\n",
    "\n",
    "X_train, X_test = X_seq[:int(len(X_seq)*0.7)], X_seq[int(len(X_seq)*0.7):]\n",
    "y_train, y_test = y_seq[:int(len(y_seq)*0.7)], y_seq[int(len(y_seq)*0.7):]\n",
    "\n",
    "X1_train, X1_test = X1_seq[:int(len(X1_seq)*0.7)], X1_seq[int(len(X1_seq)*0.7):]\n",
    "y1_train, y1_test = y1_seq[:int(len(y1_seq)*0.7)], y1_seq[int(len(y1_seq)*0.7):]\n",
    "\n",
    "# 0.6, 0.2, 0.2씩 train, val, test로 분리하기\n",
    "\n",
    "X_train_cat = torch.cat([X_train, X1_train], dim = 0)\n",
    "y_train_cat = torch.cat([y_train, y1_train], dim = 0)\n",
    "X_test_cat = torch.cat([X_test, X1_test], dim = 0)\n",
    "y_test_cat = torch.cat([y_test, y1_test], dim = 0)\n",
    "\n",
    "\n",
    "train_DS = TensorDataset(X_train_cat, y_train_cat)\n",
    "test_DS = TensorDataset(X_test_cat, y_test_cat)\n",
    "\n",
    "\n",
    "train_DL = DataLoader(train_DS, batch_size = batch_size)\n",
    "test_DL = DataLoader(test_DS, batch_size = batch_size)\n",
    "\n",
    "\n",
    "# 0.6, 0.2, 0.2씩 train, val, test로 분리하기\n",
    "\n",
    "\n",
    "# batch_size에 따라 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 10, 11])\n",
      "torch.Size([36, 1])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_DL))\n",
    "print(data[0].shape)\n",
    "print(data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension = 64\n",
    "# to_query = nn.Linear(dimension,dimension)\n",
    "# to_key = nn.Linear(dimension, dimension)\n",
    "# to_value = nn.Linear(dimension, dimension)\n",
    "\n",
    "# query = to_query(X_train)\n",
    "# key = to_key(X_train)\n",
    "# value = to_value(X_train)\n",
    "\n",
    "# print(f'Query : {query.shape}')\n",
    "# print(f'Key : {key.shape}')\n",
    "# print(f'Value : {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_score = query @ key.permute(0, 1, 3, 2)\n",
    "# attention_score = attention_score.softmax(dim = -1)\n",
    "\n",
    "# contextualized_tokens = attention_score @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size, hidden_size, batch_first = True).to(self.device)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, hidden = self.LSTM(input) # input: 36, 10, 16/ output: 36, 10, 16 / hidden: 1, 36, 16\n",
    "        \n",
    "        return output, hidden \n",
    "    \n",
    "        \n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, device):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, batch_first = True).to(self.device)\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.LSTM(input, hidden) \n",
    "        output = self.output_linear(output) # output: 36, 10, 1 / hidden: 1, 36, 16\n",
    "        return output, hidden\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, device).to(self.device)\n",
    "        self.decoder = LSTMDecoder(hidden_size, output_size, device).to(self.device)\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_size + output_size, sequence_length).to(self.device)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, sequence_length, input_size) # input: 36, 10, 16\n",
    "        encoder_output, encoder_hidden = self.encoder(input) # output: 36, 10, 16 / hidden: 1, 36, 16\n",
    "        \n",
    "        hidden = encoder_hidden\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(sequence_length):\n",
    "            decoder_input = encoder_output[:, i:i+1, :] # 36, 1, 16\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden) \n",
    "            # output: 36, 1, 1 / hidden: 1, 36, 16\n",
    "            \n",
    "            attn_input = torch.cat((decoder_output, encoder_output[:,i:i+1,:]), dim=2) # input : 36, 1, 17\n",
    "            attn_weights = torch.softmax(self.attention(attn_input), dim=2) # 36, 1, 10\n",
    "            context = torch.bmm(attn_weights, encoder_output) # 36, 1, 10 @ 36, 10, 16\n",
    "            # 36, 1, 16\n",
    "            decoder_output = torch.cat((decoder_output, context), dim=2)\n",
    "            outputs.append(decoder_output)\n",
    "            \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AML2\\Desktop\\TIL\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([36, 1])) that is different to the input size (torch.Size([36, 10, 17])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (36) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[199], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m seq, target \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m out \u001b[39m=\u001b[39m model(seq)\n\u001b[1;32m---> 16\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, target)\n\u001b[0;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\AML2\\Desktop\\TIL\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\AML2\\Desktop\\TIL\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\AML2\\Desktop\\TIL\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\AML2\\Desktop\\TIL\\venv\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (36) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #gpu 활성화 확인\n",
    "model = Attention(input_size, hidden_size, output_size, device).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "loss_graph = []\n",
    "n = len(train_DL)\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0\n",
    "    for data in train_DL:\n",
    "        seq, target = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        out = model(seq)\n",
    "        loss = criterion(out, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    loss_graph.append(running_loss/n)\n",
    "    if epoch % 20 == 0:\n",
    "        print(\"[epoch: %d] loss : %.4f\" %(epoch,running_loss/n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
